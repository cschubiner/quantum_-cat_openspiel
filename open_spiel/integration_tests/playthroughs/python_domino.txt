game: python_domino

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.IMPERFECT_INFORMATION
GameType.long_name = "Python domino"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = []
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = True
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = True
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "python_domino"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 8
PolicyTensorShape() = [8]
MaxChanceOutcomes() = 28
GetParameters() = {}
NumPlayers() = 2
MinUtility() = -69.0
MaxUtility() = 69.0
UtilitySum() = 0.0
InformationStateTensorShape() = player: [2], hand: [7, 2], history: [14, 6]
InformationStateTensorLayout() = TensorLayout.CHW
InformationStateTensorSize() = 100
ObservationTensorShape() = player: [2], hand: [7, 2], last_move: [6], hand_sizes: [2]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 24
MaxGameLength() = 30
ToString() = "python_domino()"

# State 0
# hand0:[], hand1:[], history:[]
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "p0 hand:[] history:[]"
InformationStateString(1) = "p1 hand:[] history:[]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand: ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
InformationStateTensor(0).history: ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand: ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
InformationStateTensor(1).history: ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
ObservationString(0) = "p0 hand:[]"
ObservationString(1) = "p1 hand:[]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[]"
PrivateObservationString(1) = "p1 hand:[]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand: ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
ObservationTensor(0).last_move: ◯◯◯◯◯◯
ObservationTensor(0).hand_sizes: ◯◯
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand: ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
ObservationTensor(1).last_move: ◯◯◯◯◯◯
ObservationTensor(1).hand_sizes: ◯◯
ChanceOutcomes() = [(0, 0.03571428571428571), (1, 0.03571428571428571), (2, 0.03571428571428571), (3, 0.03571428571428571), (4, 0.03571428571428571), (5, 0.03571428571428571), (6, 0.03571428571428571), (7, 0.03571428571428571), (8, 0.03571428571428571), (9, 0.03571428571428571), (10, 0.03571428571428571), (11, 0.03571428571428571), (12, 0.03571428571428571), (13, 0.03571428571428571), (14, 0.03571428571428571), (15, 0.03571428571428571), (16, 0.03571428571428571), (17, 0.03571428571428571), (18, 0.03571428571428571), (19, 0.03571428571428571), (20, 0.03571428571428571), (21, 0.03571428571428571), (22, 0.03571428571428571), (23, 0.03571428571428571), (24, 0.03571428571428571), (25, 0.03571428571428571), (26, 0.03571428571428571), (27, 0.03571428571428571)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
StringLegalActions() = ["Deal (3, 4)", "Deal (4, 6)", "Deal (0, 2)", "Deal (0, 5)", "Deal (2, 2)", "Deal (1, 6)", "Deal (2, 5)", "Deal (1, 3)", "Deal (4, 5)", "Deal (3, 3)", "Deal (5, 6)", "Deal (3, 6)", "Deal (0, 1)", "Deal (2, 4)", "Deal (1, 2)", "Deal (0, 4)", "Deal (1, 5)", "Deal (3, 5)", "Deal (4, 4)", "Deal (5, 5)", "Deal (0, 0)", "Deal (1, 1)", "Deal (0, 3)", "Deal (1, 4)", "Deal (0, 6)", "Deal (2, 3)", "Deal (2, 6)", "Deal (6, 6)"]

# Apply action "Deal (3, 4)"
action: 0

# State 1
# hand0:['(3, 4)'], hand1:[], history:[]
IsTerminal() = False
History() = [0]
HistoryString() = "0"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "p0 hand:[(3, 4)] history:[]"
InformationStateString(1) = "p1 hand:[] history:[]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [3.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).history: ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand: ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
InformationStateTensor(1).history: ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
ObservationString(0) = "p0 hand:[(3, 4)]"
ObservationString(1) = "p1 hand:[]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(3, 4)]"
PrivateObservationString(1) = "p1 hand:[]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [3.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).last_move: ◯◯◯◯◯◯
ObservationTensor(0).hand_sizes: ◉◯
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand: ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
ObservationTensor(1).last_move: ◯◯◯◯◯◯
ObservationTensor(1).hand_sizes: ◯◉
ChanceOutcomes() = [(0, 0.037037037037037035), (1, 0.037037037037037035), (2, 0.037037037037037035), (3, 0.037037037037037035), (4, 0.037037037037037035), (5, 0.037037037037037035), (6, 0.037037037037037035), (7, 0.037037037037037035), (8, 0.037037037037037035), (9, 0.037037037037037035), (10, 0.037037037037037035), (11, 0.037037037037037035), (12, 0.037037037037037035), (13, 0.037037037037037035), (14, 0.037037037037037035), (15, 0.037037037037037035), (16, 0.037037037037037035), (17, 0.037037037037037035), (18, 0.037037037037037035), (19, 0.037037037037037035), (20, 0.037037037037037035), (21, 0.037037037037037035), (22, 0.037037037037037035), (23, 0.037037037037037035), (24, 0.037037037037037035), (25, 0.037037037037037035), (26, 0.037037037037037035)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
StringLegalActions() = ["Deal (4, 6)", "Deal (0, 2)", "Deal (0, 5)", "Deal (2, 2)", "Deal (1, 6)", "Deal (2, 5)", "Deal (1, 3)", "Deal (4, 5)", "Deal (3, 3)", "Deal (5, 6)", "Deal (3, 6)", "Deal (0, 1)", "Deal (2, 4)", "Deal (1, 2)", "Deal (0, 4)", "Deal (1, 5)", "Deal (3, 5)", "Deal (4, 4)", "Deal (5, 5)", "Deal (0, 0)", "Deal (1, 1)", "Deal (0, 3)", "Deal (1, 4)", "Deal (0, 6)", "Deal (2, 3)", "Deal (2, 6)", "Deal (6, 6)"]

# Apply action "Deal (2, 3)"
action: 24

# State 2
# Apply action "Deal (1, 6)"
action: 4

# State 3
# Apply action "Deal (6, 6)"
action: 24

# State 4
# Apply action "Deal (2, 6)"
action: 23

# State 5
# Apply action "Deal (4, 6)"
action: 0

# State 6
# Apply action "Deal (2, 5)"
action: 3

# State 7
# Apply action "Deal (4, 5)"
action: 4

# State 8
# Apply action "Deal (1, 4)"
action: 18

# State 9
# Apply action "Deal (5, 5)"
action: 14

# State 10
# Apply action "Deal (3, 5)"
action: 12

# State 11
# Apply action "Deal (1, 3)"
action: 3

# State 12
# Apply action "Deal (2, 4)"
action: 7

# State 13
# Apply action "Deal (0, 0)"
action: 11

# State 14
# hand0:['(3, 4)', '(2, 3)', '(1, 6)', '(6, 6)', '(2, 6)', '(4, 6)', '(2, 5)'], hand1:['(4, 5)', '(1, 4)', '(5, 5)', '(3, 5)', '(1, 3)', '(2, 4)', '(0, 0)'], history:[]
IsTerminal() = False
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "p0 hand:[(3, 4), (2, 3), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)] history:[]"
InformationStateString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (2, 4), (0, 0)] history:[]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [3.0, 4.0, 2.0, 3.0, 1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0]
InformationStateTensor(0).history: ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 2.0, 4.0, 0.0, 0.0]
InformationStateTensor(1).history: ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
                                   ◯◯◯◯◯◯
ObservationString(0) = "p0 hand:[(3, 4), (2, 3), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
ObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (2, 4), (0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(3, 4), (2, 3), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
PrivateObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (2, 4), (0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [3.0, 4.0, 2.0, 3.0, 1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0]
ObservationTensor(0).last_move: ◯◯◯◯◯◯
ObservationTensor(0).hand_sizes = [7.0, 7.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 2.0, 4.0, 0.0, 0.0]
ObservationTensor(1).last_move: ◯◯◯◯◯◯
ObservationTensor(1).hand_sizes = [7.0, 7.0]
Rewards() = [0, 0]
Returns() = [-56, 56]
LegalActions() = [0, 1, 2, 3, 4, 5, 6]
StringLegalActions() = ["p0 | tile=(3, 4) | pip=None | new_edges=[3, 4]", "p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]", "p0 | tile=(1, 6) | pip=None | new_edges=[1, 6]", "p0 | tile=(6, 6) | pip=None | new_edges=[6, 6]", "p0 | tile=(2, 6) | pip=None | new_edges=[2, 6]", "p0 | tile=(4, 6) | pip=None | new_edges=[4, 6]", "p0 | tile=(2, 5) | pip=None | new_edges=[2, 5]"]

# Apply action "p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]"
action: 1

# State 15
# hand0:['(3, 4)', '(1, 6)', '(6, 6)', '(2, 6)', '(4, 6)', '(2, 5)'], hand1:['(4, 5)', '(1, 4)', '(5, 5)', '(3, 5)', '(1, 3)', '(2, 4)', '(0, 0)'], history:['p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]']
IsTerminal() = False
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "p0 hand:[(3, 4), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]]"
InformationStateString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (2, 4), (0, 0)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [3.0, 4.0, 1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0]
InformationStateTensor(0).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 2.0, 4.0, 0.0, 0.0]
InformationStateTensor(1).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationString(0) = "p0 hand:[(3, 4), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
ObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (2, 4), (0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(3, 4), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
PrivateObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (2, 4), (0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [3.0, 4.0, 1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0]
ObservationTensor(0).last_move = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0]
ObservationTensor(0).hand_sizes = [6.0, 7.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 2.0, 4.0, 0.0, 0.0]
ObservationTensor(1).last_move = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0]
ObservationTensor(1).hand_sizes = [7.0, 6.0]
Rewards() = [0, 0]
Returns() = [-51, 51]
LegalActions() = [0, 1, 2]
StringLegalActions() = ["p1 | tile=(3, 5) | pip=3 | new_edges=[2, 5]", "p1 | tile=(1, 3) | pip=3 | new_edges=[1, 2]", "p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]"]

# Apply action "p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]"
action: 2

# State 16
# hand0:['(3, 4)', '(1, 6)', '(6, 6)', '(2, 6)', '(4, 6)', '(2, 5)'], hand1:['(4, 5)', '(1, 4)', '(5, 5)', '(3, 5)', '(1, 3)', '(0, 0)'], history:['p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]', 'p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]']
IsTerminal() = False
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "p0 hand:[(3, 4), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]]"
InformationStateString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (0, 0)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [3.0, 4.0, 1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0]
InformationStateTensor(0).history = [2.0, 3.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).history = [2.0, 3.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationString(0) = "p0 hand:[(3, 4), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
ObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(3, 4), (1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
PrivateObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [3.0, 4.0, 1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0]
ObservationTensor(0).last_move = [2.0, 4.0, 3.0, 4.0, 0.0, 1.0]
ObservationTensor(0).hand_sizes = [6.0, 6.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).last_move = [2.0, 4.0, 3.0, 4.0, 0.0, 1.0]
ObservationTensor(1).hand_sizes = [6.0, 6.0]
Rewards() = [0, 0]
Returns() = [-51, 51]
LegalActions() = [0, 1, 2]
StringLegalActions() = ["p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]", "p0 | tile=(3, 4) | pip=4 | new_edges=[3, 3]", "p0 | tile=(4, 6) | pip=4 | new_edges=[3, 6]"]

# Apply action "p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]"
action: 0

# State 17
# hand0:['(1, 6)', '(6, 6)', '(2, 6)', '(4, 6)', '(2, 5)'], hand1:['(4, 5)', '(1, 4)', '(5, 5)', '(3, 5)', '(1, 3)', '(0, 0)'], history:['p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]', 'p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]', 'p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]']
IsTerminal() = False
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "p0 hand:[(1, 6), (6, 6), (2, 6), (4, 6), (2, 5)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]]"
InformationStateString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (0, 0)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationString(0) = "p0 hand:[(1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
ObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
PrivateObservationString(1) = "p1 hand:[(4, 5), (1, 4), (5, 5), (3, 5), (1, 3), (0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).last_move = [3.0, 4.0, 4.0, 4.0, 0.0, 1.0]
ObservationTensor(0).hand_sizes = [5.0, 6.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand = [4.0, 5.0, 1.0, 4.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).last_move = [3.0, 4.0, 4.0, 4.0, 0.0, 1.0]
ObservationTensor(1).hand_sizes = [6.0, 5.0]
Rewards() = [0, 0]
Returns() = [-44, 44]
LegalActions() = [0, 1]
StringLegalActions() = ["p1 | tile=(4, 5) | pip=4 | new_edges=[4, 5]", "p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]"]

# Apply action "p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]"
action: 1

# State 18
# hand0:['(1, 6)', '(6, 6)', '(2, 6)', '(4, 6)', '(2, 5)'], hand1:['(4, 5)', '(5, 5)', '(3, 5)', '(1, 3)', '(0, 0)'], history:['p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]', 'p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]', 'p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]', 'p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]']
IsTerminal() = False
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0, 1]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0, 1"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "p0 hand:[(1, 6), (6, 6), (2, 6), (4, 6), (2, 5)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4], p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]]"
InformationStateString(1) = "p1 hand:[(4, 5), (5, 5), (3, 5), (1, 3), (0, 0)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4], p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).history = [2.0, 3.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 3.0, 4.0, 4.0, 4.0, 1.0, 1.0, 1.0, 4.0, 1.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand = [4.0, 5.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).history = [2.0, 3.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 3.0, 4.0, 4.0, 4.0, 1.0, 1.0, 1.0, 4.0, 1.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationString(0) = "p0 hand:[(1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
ObservationString(1) = "p1 hand:[(4, 5), (5, 5), (3, 5), (1, 3), (0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(1, 6), (6, 6), (2, 6), (4, 6), (2, 5)]"
PrivateObservationString(1) = "p1 hand:[(4, 5), (5, 5), (3, 5), (1, 3), (0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [1.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).last_move = [1.0, 4.0, 1.0, 4.0, 0.0, 1.0]
ObservationTensor(0).hand_sizes = [5.0, 5.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand = [4.0, 5.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).last_move = [1.0, 4.0, 1.0, 4.0, 0.0, 1.0]
ObservationTensor(1).hand_sizes = [5.0, 5.0]
Rewards() = [0, 0]
Returns() = [-44, 44]
LegalActions() = [0, 1]
StringLegalActions() = ["p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6]", "p0 | tile=(4, 6) | pip=4 | new_edges=[1, 6]"]

# Apply action "p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6]"
action: 0

# State 19
# hand0:['(6, 6)', '(2, 6)', '(4, 6)', '(2, 5)'], hand1:['(4, 5)', '(5, 5)', '(3, 5)', '(1, 3)', '(0, 0)'], history:['p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]', 'p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]', 'p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]', 'p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]', 'p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6]']
IsTerminal() = False
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0, 1, 0]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0, 1, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "p0 hand:[(6, 6), (2, 6), (4, 6), (2, 5)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4], p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4], p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6]]"
InformationStateString(1) = "p1 hand:[(4, 5), (5, 5), (3, 5), (1, 3), (0, 0)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4], p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4], p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6]]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 0.0, 1.0, 1.0, 4.0, 1.0, 4.0, 1.0, 1.0, 1.0, 6.0, 4.0, 6.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand = [4.0, 5.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 0.0, 1.0, 1.0, 4.0, 1.0, 4.0, 1.0, 1.0, 1.0, 6.0, 4.0, 6.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationString(0) = "p0 hand:[(6, 6), (2, 6), (4, 6), (2, 5)]"
ObservationString(1) = "p1 hand:[(4, 5), (5, 5), (3, 5), (1, 3), (0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(6, 6), (2, 6), (4, 6), (2, 5)]"
PrivateObservationString(1) = "p1 hand:[(4, 5), (5, 5), (3, 5), (1, 3), (0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [6.0, 6.0, 2.0, 6.0, 4.0, 6.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).last_move = [1.0, 6.0, 4.0, 6.0, 0.0, 1.0]
ObservationTensor(0).hand_sizes = [4.0, 5.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand = [4.0, 5.0, 5.0, 5.0, 3.0, 5.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).last_move = [1.0, 6.0, 4.0, 6.0, 0.0, 1.0]
ObservationTensor(1).hand_sizes = [5.0, 4.0]
Rewards() = [0, 0]
Returns() = [-37, 37]
LegalActions() = [0]
StringLegalActions() = ["p1 | tile=(4, 5) | pip=4 | new_edges=[5, 6]"]

# Apply action "p1 | tile=(4, 5) | pip=4 | new_edges=[5, 6]"
action: 0

# State 20
# Apply action "p0 | tile=(2, 6) | pip=6 | new_edges=[2, 5]"
action: 1

# State 21
# Apply action "p1 | tile=(3, 5) | pip=5 | new_edges=[2, 3]"
action: 1

# State 22
# Apply action "p0 | tile=(2, 5) | pip=2 | new_edges=[3, 5]"
action: 0

# State 23
# Apply action "p1 | tile=(5, 5) | pip=5 | new_edges=[3, 5]"
action: 0

# State 24
# Apply action "p1 | tile=(1, 3) | pip=3 | new_edges=[1, 5]"
action: 0

# State 25
# hand0:['(6, 6)', '(4, 6)'], hand1:['(0, 0)'], history:['p0 | tile=(2, 3) | pip=None | new_edges=[2, 3]', 'p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4]', 'p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4]', 'p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4]', 'p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6]', 'p1 | tile=(4, 5) | pip=4 | new_edges=[5, 6]', 'p0 | tile=(2, 6) | pip=6 | new_edges=[2, 5]', 'p1 | tile=(3, 5) | pip=5 | new_edges=[2, 3]', 'p0 | tile=(2, 5) | pip=2 | new_edges=[3, 5]', 'p1 | tile=(5, 5) | pip=5 | new_edges=[3, 5]', 'p1 | tile=(1, 3) | pip=3 | new_edges=[1, 5]']
IsTerminal() = True
History() = [0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0]
HistoryString() = "0, 24, 4, 24, 23, 0, 3, 4, 18, 14, 12, 3, 7, 11, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.TERMINAL
InformationStateString(0) = "p0 hand:[(6, 6), (4, 6)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4], p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4], p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6], p1 | tile=(4, 5) | pip=4 | new_edges=[5, 6], p0 | tile=(2, 6) | pip=6 | new_edges=[2, 5], p1 | tile=(3, 5) | pip=5 | new_edges=[2, 3], p0 | tile=(2, 5) | pip=2 | new_edges=[3, 5], p1 | tile=(5, 5) | pip=5 | new_edges=[3, 5], p1 | tile=(1, 3) | pip=3 | new_edges=[1, 5]]"
InformationStateString(1) = "p1 hand:[(0, 0)] history:[p0 | tile=(2, 3) | pip=None | new_edges=[2, 3], p1 | tile=(2, 4) | pip=2 | new_edges=[3, 4], p0 | tile=(3, 4) | pip=3 | new_edges=[4, 4], p1 | tile=(1, 4) | pip=4 | new_edges=[1, 4], p0 | tile=(1, 6) | pip=1 | new_edges=[4, 6], p1 | tile=(4, 5) | pip=4 | new_edges=[5, 6], p0 | tile=(2, 6) | pip=6 | new_edges=[2, 5], p1 | tile=(3, 5) | pip=5 | new_edges=[2, 3], p0 | tile=(2, 5) | pip=2 | new_edges=[3, 5], p1 | tile=(5, 5) | pip=5 | new_edges=[3, 5], p1 | tile=(1, 3) | pip=3 | new_edges=[1, 5]]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).hand = [6.0, 6.0, 4.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 3.0, 4.0, 4.0, 4.0, 0.0, 1.0, 1.0, 4.0, 1.0, 4.0, 0.0, 1.0, 1.0, 6.0, 4.0, 6.0, 0.0, 1.0, 4.0, 5.0, 5.0, 6.0, 0.0, 1.0, 2.0, 6.0, 2.0, 5.0, 0.0, 1.0, 3.0, 5.0, 2.0, 3.0, 0.0, 1.0, 2.0, 5.0, 3.0, 5.0, 0.0, 1.0, 5.0, 5.0, 3.0, 5.0, 0.0, 1.0, 1.0, 3.0, 1.0, 5.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).hand: ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
                                ◯◯
InformationStateTensor(1).history = [2.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 3.0, 4.0, 4.0, 4.0, 0.0, 1.0, 1.0, 4.0, 1.0, 4.0, 0.0, 1.0, 1.0, 6.0, 4.0, 6.0, 0.0, 1.0, 4.0, 5.0, 5.0, 6.0, 0.0, 1.0, 2.0, 6.0, 2.0, 5.0, 0.0, 1.0, 3.0, 5.0, 2.0, 3.0, 0.0, 1.0, 2.0, 5.0, 3.0, 5.0, 0.0, 1.0, 5.0, 5.0, 3.0, 5.0, 0.0, 1.0, 1.0, 3.0, 1.0, 5.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationString(0) = "p0 hand:[(6, 6), (4, 6)]"
ObservationString(1) = "p1 hand:[(0, 0)]"
PublicObservationString() = "p0"
PrivateObservationString(0) = "p0 hand:[(6, 6), (4, 6)]"
PrivateObservationString(1) = "p1 hand:[(0, 0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).hand = [6.0, 6.0, 4.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).last_move = [1.0, 3.0, 1.0, 5.0, 0.0, 1.0]
ObservationTensor(0).hand_sizes = [2.0, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).hand: ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
                           ◯◯
ObservationTensor(1).last_move = [1.0, 3.0, 1.0, 5.0, 0.0, 1.0]
ObservationTensor(1).hand_sizes = [1.0, 2.0]
Rewards() = [-22, 22]
Returns() = [-22, 22]
